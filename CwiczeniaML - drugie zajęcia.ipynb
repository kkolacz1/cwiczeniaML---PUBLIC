{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c043a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98c978",
   "metadata": {},
   "source": [
    "## Regularyzacja \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59987eb3",
   "metadata": {},
   "source": [
    "Wprowadzenie szerszej klasy funkcji bazowych jak i ilości parametrów może bardzo szybko prowadzić do zjawiska overfittingu (przetrenowania). Jak sobie z tym poradzić? Na przykład, wprowadzając pewne dodatkowe obciążenie do funkcji kosztu zależne od współczynników:\n",
    "\n",
    "Regularyzacja L2 (Ridge Regression): LF+ = alpha * sum wi^2\n",
    "\n",
    "Regularyzacja L1 (Lasso Regression): LF+ = alpha * sum|wi|\n",
    "    \n",
    "    tzn. LFnew = LF + LF+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5bd4f",
   "metadata": {},
   "source": [
    "## Zadanie1: \n",
    "Zdefiniuj klasę implementującą regresję liniową z regularyzacją L1/L2 dla dowolnej liczby zmiennych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36801d",
   "metadata": {},
   "source": [
    "## Zadanie2:\n",
    "Wybierz dowolny zbiór danych lub wygeneruj syntetyczne dane. Zastosuj model regresji liniowej z regularyzacją L1. Na podstawie zbioru walidacyjnego wybierz optymalne parametry oraz (rozważ chociaż 15-20 różnych kombinacji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "401f03e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3485879314.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [25]\u001b[1;36m\u001b[0m\n\u001b[1;33m    match self.regression:\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "# zadanie 1 \n",
    "###################################################################################################\n",
    "class LinearModel2v:\n",
    "    def __init__(self, eta=0.001, diff=0.001, w1=1, w2=1, w0=1, maxiter=1000, alfa=0, regression='L1'):\n",
    "        self.eta = eta\n",
    "        self.diff = diff\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.w0 = w0\n",
    "        self.maxiter = maxiter\n",
    "        self.alfa = alfa\n",
    "        self.regression = regression  # argument responsible for regularization choice. Possible options: 'L0' 'L1' 'L2'\n",
    "\n",
    "    def loss_function(self, X, t):  # loss_function responsible for determining regression score\n",
    "        N = len(X)\n",
    "        C = 0\n",
    "        for i in range(N):\n",
    "            C += (X[i][0] * self.w1 + X[i][1] * self.w2 + self.w0 - t[i]) ** 2\n",
    "            # if math.isinf(C) or math.isnan(C):\n",
    "            # breakpoint()\n",
    "\n",
    "        # now adding LF+ parameter responsible for regularization\n",
    "        match self.regression:  \n",
    "\n",
    "            case 'L1':\n",
    "                lfp = self.alfa * (abs(self.w1) + abs(self.w2) + abs(self.w0))\n",
    "            case 'L2':\n",
    "                lfp = self.alfa * (self.w1**2 + self.w2**2 + self.w0**2)\n",
    "            case _:\n",
    "                lfp = 0\n",
    "\n",
    "        return C / (2 * N) + lfp\n",
    "\n",
    "    def update_weights(self, X, t):  # function responsible for updating model weights, will break for large 'eta' values\n",
    "        N = len(X)\n",
    "        dC1 = 0\n",
    "        dC2 = 0\n",
    "        dC0 = 0\n",
    "        for i in range(N):\n",
    "            y_pred = X[i][0] * self.w1 + X[i][1] * self.w2 + self.w0\n",
    "            dC1 += 2 * X[i][0] * (y_pred - t[i])\n",
    "            dC2 += 2 * X[i][1] * (y_pred - t[i])\n",
    "            dC0 += 2 * (y_pred - t[i])\n",
    "\n",
    "        self.w1 = self.w1 - self.eta * dC1 / (2 * N)\n",
    "        self.w2 = self.w2 - self.eta * dC2 / (2 * N)\n",
    "        self.w0 = self.w0 - self.eta * dC0 / (2 * N)\n",
    "\n",
    "        # if self.w1 > 1e100:\n",
    "        #     breakpoint()\n",
    "\n",
    "    def train(self, X, t):  # calls update_weights, until difference in loss_function is < diff or limit is reached\n",
    "        l = []\n",
    "        ile = 0\n",
    "        while True:\n",
    "            l.append(self.loss_function(X, t))\n",
    "            self.update_weights(X, t)\n",
    "            ile += 1\n",
    "            if len(l) > 2:\n",
    "                if abs(l[-1] - l[-2]) / l[-1] < self.diff or ile > self.maxiter:\n",
    "                    break\n",
    "##################################################################################                    \n",
    "# zadanie 2\n",
    "##################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "\n",
    "# imports ML data from boston dataset\n",
    "boston_data = load_boston()\n",
    "d = boston_data['data']\n",
    "d2 = d[:, [2,5]] #wyciągamy tylko 2 cechy: INDUS, RM\n",
    "target = boston_data['target']\n",
    "\n",
    "# split dataset into training and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(d2, target, test_size=0.4, random_state=42)\n",
    "X_walid, X_test, y_walid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# define testing values for ni and alfa\n",
    "tested_ni = list(np.logspace(-5, 0, num=30))\n",
    "tested_alfa = list(np.logspace(-10, 1, num=10))\n",
    "\n",
    "# create a combination array, which will allow to create a 3d optimization map\n",
    "combo = list(itertools.product(tested_ni, tested_alfa))\n",
    "\n",
    "# test combinations of parameters, and save loss_function into 'result' array\n",
    "# code will result in overflow in cases where linear model does not converge\n",
    "result = []\n",
    "for i in combo:\n",
    "    model = LinearModel2v(eta=i[0], alfa=i[1])\n",
    "    model.train(X_walid, y_walid)\n",
    "    result.append(model.loss_function(X_walid, y_walid))\n",
    "\n",
    "# print the combination of parameters which achieved the best score\n",
    "print(f'Najmniejsza wartosc funkcji kosztu otrzymano dla eta = {combo[result.index(min(result))][0]}, alfa = {combo[result.index(min(result))][1]} ')\n",
    "\n",
    "# plot the 3d surface map\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for idx, x in enumerate(result):\n",
    "    ax.scatter(np.log10(combo[idx][0]), np.log10(combo[idx][1]), result[idx])\n",
    "\n",
    "plt.xlabel('log10 eta')\n",
    "plt.ylabel('log10 alfa')\n",
    "ax.set_zlabel('Wartosc funkcji kosztu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8cb135",
   "metadata": {},
   "source": [
    "## Regresja logistyczna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97799939",
   "metadata": {},
   "source": [
    "## Zadanie3: \n",
    "Uzasadnij ponizsze wartości prawdopodobieństw w oparciu o parametry modelu.\n",
    "\n",
    "Zaimplementuj klase dla regresji logistycznej wraz z regularyzacją L1 i L2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed6b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obliczone prawdopodobienstwo otrzymania '1' dla podanych X1 X2 to: 0.02802931602884544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "X = np.array([[1,2],[2,3],[4,5],[1,-4],[5,-7],[-3,-1]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "# model.coef_ Wspolczynniki kierunkowe\n",
    "# output = array[-0.39682866, -0.82497163]\n",
    "\n",
    "# model.intercept_ Wyraz wolny\n",
    "# output = array([0.11932619]\n",
    "\n",
    "# model.predict([[3,3]]) Predykcja klasy\n",
    "# output = array[0]\n",
    "\n",
    "# model.predict_proba([3,3]) Obliczone prawdopodobienstwa\n",
    "# output = array([[0.97197068, 0.02802932]]))\n",
    "\n",
    "###########################\n",
    "\n",
    "# I'm not sure what's the best way to justify prediction output. I guess I'll do it by calculating the output manually?\n",
    "X1 = 3\n",
    "X2 = 3\n",
    "print(f'obliczone prawdopodobienstwo otrzymania \\'1\\' dla podanych X1 X2 to: {1/(1 + math.exp(-(X1*-0.39682866 + X2*-0.82497163 + 0.11932619)))}')\n",
    "# plt.scatter(X[:,0],X[:,1] )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95849c",
   "metadata": {},
   "source": [
    "## Regresja logistyczna wieloklasowa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d865ad",
   "metadata": {},
   "source": [
    "## Zadanie4: \n",
    "Wyznacz powyzsze prawdopodobieństwa ręcznie na podstawie parametrów modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bda34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,)\n",
      "Counter({1: 334, 2: 334, 0: 332})\n",
      "[[ 0.01577681 -0.05318977  0.21160694  0.14655186 -0.24476592 -0.00318551\n",
      "  -0.20914917 -0.26505038 -0.02765106  0.26778537]\n",
      " [ 0.20327639  0.0990516  -0.39851296 -0.1045002   0.02344135 -0.06340171\n",
      "   0.19494493  0.54495946 -0.18417782 -0.2844254 ]\n",
      " [-0.2190532  -0.04586183  0.18690602 -0.04205165  0.22132456  0.06658722\n",
      "   0.01420424 -0.27990908  0.21182888  0.01664003]] [ 0.0294703  -0.26754233  0.23807203]\n",
      "Przewidziana klasa: 1\n",
      "Przewidziane prawdopodobieństwa: [0.16470456 0.50297138 0.33232406]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) #generating data\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(Counter(y))\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_, model.intercept_) #printuje parametry modelu\n",
    "\n",
    "#nowa obserwacja\n",
    "row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426]\n",
    "#przewiduje jej klasę\n",
    "yhat = model.predict([row])\n",
    "print(f'Przewidziana klasa: {yhat[0]}')\n",
    "\n",
    "#przewiduje prawdopodobienstwa\n",
    "yhat = model.predict_proba([row])\n",
    "print(f'Przewidziane prawdopodobieństwa: {yhat[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb3c51",
   "metadata": {},
   "source": [
    "## Zadanie5: \n",
    "Wyjaśnij powyższy wykres? Co oznacza to rozgałęzienie?\n",
    "\n",
    "(poniżej komentarz do wykresu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d215be8",
   "metadata": {},
   "source": [
    "* Wykres przedstawia wplyw stezenia modelowanego skladnika na klase predykcji.\n",
    "* Ujemny wspolczynnik wagi dla wyzszych stezen sugeruje, ze wysokie stezenie dla danego skladnika ma negatywny wplyw na szanse na przypisanie do modelowanej klasy\n",
    "* Reczna ocena wykresu zdaje sie potwierdzac ta hipoteze - wysoka zawartosc popiolu, alkoholu czy aminokwasow sugerowalaby nizsza jakosc wina"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
