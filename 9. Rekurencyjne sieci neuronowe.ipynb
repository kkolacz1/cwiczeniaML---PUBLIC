{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P95c6hK3hAQq"
   },
   "source": [
    "# Rekurencyjne Sieci Neuronowe (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Materiały w większości pochodzą od pracowników wydziału matematyki i informatyki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laVdd5g5hAQu"
   },
   "source": [
    "### Importy i Utilsy  (odpalić i schować)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I0D3yk7lhAQu"
   },
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "all_letters = string.ascii_letters\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, targets):\n",
    "        \n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self, ind):\n",
    "        \n",
    "        return self.data[ind], self.targets[ind]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    \n",
    "def unicode_to__ascii(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
    "                                                                 and c in all_letters)\n",
    "                   \n",
    "\n",
    "def read_lines(filename: str) -> List[str]:\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicode_to__ascii(line) for line in lines]\n",
    "\n",
    "\n",
    "def letter_to_index(letter: str) -> int:\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def line_to_tensor(line: str) -> torch.Tensor:\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for i, letter in enumerate(line):\n",
    "        tensor[i][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcSQvaMPhAQv"
   },
   "source": [
    "## Dane sekwencyjne\n",
    "\n",
    "Modele, którymi zajmowaliśmy się wcześniej zakładały konkretny kształt danych. Dla przykładu klasyczna sieć neuronowa fully-connected dla MNISTa zakładała, że na wejściu dostanie wektory rozmiaru 784 - dla wektorów o innej wymiarowości i innych obiektów model zwyczajnie nie będzie działać.\n",
    "\n",
    "Takie założenie bywa szczególnie niewygodne przy pracy z niektórymi typami danych, takimi jak:\n",
    "* językiem naturalny (słowa czy zdania mają zadanej z góry liczby znaków)\n",
    "* szeregi czasowe (dane giełdowe ciągną się właściwie w nieskończoność) \n",
    "* dźwięk (nagrania mogą być krótsze lub dłuższe).\n",
    "\n",
    "Do rozwiązania tego problemu służą rekuencyjne sieci neuronowe (*recurrent neural networks, RNNs*), które zapamiętują swój stan z poprzedniej iteracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH3chO87hAQv"
   },
   "source": [
    "### Ładowanie danych\n",
    "Poniższe dwie komórki ściągają dataset nazwisk z 18 różnych narodowości. Każda litera w danym nazwisku jest zamieniana na jej indeks z alfabetu w postaci kodowania \"one-hot\". Inaczej mówiąc, każde nazwisko jest binarną macierzą rozmiaru `n_letters` $\\times$ `len(name)`. \n",
    "\n",
    "Dodatkowo, ponieważ ten dataset jest mocno niezbalansowany, użyjemy specjalnego samplera do losowania przykładów treningowych, tak aby do uczenia sieć widziała tyle samo przykładów z każdej klasy.\n",
    "\n",
    "Ponieważ nazwiska mogą mieć różne długości będziemy rozważać `batch_size = 1` w tym notebooku (choć implementacje modeli powinny działać dla dowolnych wartości `batch_size`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1610371885104,
     "user": {
      "displayName": "Igor Sieradzki",
      "photoUrl": "",
      "userId": "10906567088243523558"
     },
     "user_tz": -60
    },
    "id": "maOHB6NZiRgr",
    "outputId": "ddd780f4-74b9-4c06-e785-aaf5ff99e783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-07 15:39:55--  https://download.pytorch.org/tutorial/data.zip\n",
      "SSL_INIT\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 54.192.230.12, 54.192.230.105, 54.192.230.91, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|54.192.230.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2882130 (2.7M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]   2.75M  17.6MB/s    in 0.2s    \n",
      "\n",
      "2022-01-07 15:39:55 (17.6 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
      "\n",
      "unzip:  cannot find or open data.zipq, data.zipq.zip or data.zipq.ZIP.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!wget https://download.pytorch.org/tutorial/data.zip\n",
    "!unzip data.zip\n",
    "mozna tez pobrac ręcznie\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DRGjkPZ2hAQv"
   },
   "outputs": [],
   "source": [
    "# NOTE: you can change the seed or remove it completely if you like\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "data_dir = 'rnn/data/names'\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "data = []\n",
    "targets = [] \n",
    "label_to_idx = {}\n",
    "\n",
    "# read each natonality file and process data \n",
    "for label, file_name in enumerate(os.listdir(data_dir)):\n",
    "    \n",
    "    label_to_idx[label] = file_name.split('.')[0].lower()\n",
    "    \n",
    "    names = read_lines(os.path.join(data_dir, file_name))\n",
    "    data += [line_to_tensor(name) for name in names]\n",
    "    targets += len(names) * [label]\n",
    "\n",
    "# split into train and test indices\n",
    "test_frac = 0.1\n",
    "n_test = int(test_frac * len(targets))\n",
    "test_ind = np.random.choice(len(targets), size=n_test, replace=False)\n",
    "train_ind = np.setdiff1d(np.arange(len(targets)), test_ind)\n",
    "\n",
    "targets = torch.tensor(targets)\n",
    "train_targets = targets[train_ind]\n",
    "\n",
    "# calculate weights for BalancedSampler\n",
    "uni, counts = np.unique(train_targets, return_counts=True)\n",
    "weight_per_class = len(targets) / counts\n",
    "weight = [weight_per_class[c] for c in train_targets]\n",
    "# preapre the sampler\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weight, num_samples=len(weight)) \n",
    "\n",
    "train_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in train_ind], targets=train_targets)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=1, sampler=sampler)\n",
    "\n",
    "test_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in test_ind], targets=targets[test_ind])\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3205,
     "status": "ok",
     "timestamp": 1610371886790,
     "user": {
      "displayName": "Igor Sieradzki",
      "photoUrl": "",
      "userId": "10906567088243523558"
     },
     "user_tz": -60
    },
    "id": "Yvstu1-sldC6",
    "outputId": "9862286e-15c1-44b5-ed41-a91bcc7ae593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 52])\n",
      "name: Gui\n",
      "y: chinese\n"
     ]
    }
   ],
   "source": [
    "# check out the content of the dataset\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"name: \", end=\"\")\n",
    "for letter_onehot in x[0]:\n",
    "    print(all_letters[torch.argmax(letter_onehot)], end=\"\")\n",
    "\n",
    "print(\"\\ny:\", label_to_idx[y.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3VdtPOhhAQw"
   },
   "source": [
    "<h4> Zadanie 1. </h4>\n",
    "\n",
    "Zaimplementuj \"zwykłą\" sieć rekurencyjną. \n",
    "![rnn](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "\n",
    "Przyjmijmy, że stan ukryty ma wymiarowość $H$. Wtedy komórka powinna być warstwą liniową o postaci: $\\tanh(W^T [x_t, h_{t-1}] + b)$, gdzie $x_t \\in \\mathbb{R}^{D}$ to wejście w kroku $t$, $h_{t-1} \\in \\mathbb{R}^{H}$ to stan ukryty z poprzedniego kroku a $W \\in \\mathbb{R}^{(H + D) \\times H}$ i $b \\in \\mathbb{R}^H$ to parametry naszego modelu.\n",
    "\n",
    "* W klasie `RNN` należy zainicjalizować potrzebne wagi oraz zaimplementować główną logikę dla pojedynczej chwili czasowej $x_t$\n",
    "* Wyjście z sieci może mieć dowolny rozmiar, potrzebna jest również warstwa przekształacjąca $H$-wymiarowy stan ukryty na wyjście (o takiej wymiarowości ile mamy klas w naszym problemie). \n",
    "* W pętli uczenia należy dodać odpowiednie wywołanie sieci. HINT: pamiętać o iterowaniu po wymiarze \"czasowym\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WNu0vccJhAQw"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size: int,\n",
    "                 hidden_size: int, \n",
    "                 output_size: int):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "            Dimensionality of the input vector\n",
    "        :param hidden_size: int\n",
    "            Dimensionality of the hidden space\n",
    "        :param output_size: int\n",
    "            Desired dimensionality of the output vector\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.input_to_hidden = ??? \n",
    "        \n",
    "        self.hidden_to_output = ???\n",
    "    \n",
    "    # for the sake of simplicity a single forward will process only a single timestamp \n",
    "    def forward(self, \n",
    "                input: torch.tensor, \n",
    "                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        :param input: torch.tensor \n",
    "            Input tesnor for a single observation at timestep t\n",
    "            shape [batch_size, input_size]\n",
    "        :param hidden: torch.tensor\n",
    "            Representation of the memory of the RNN from previous timestep\n",
    "            shape [batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        combined = torch.cat([input, hidden], dim=1) \n",
    "        hidden = ???\n",
    "        output =  ???\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns initial value for the hidden state\n",
    "        \"\"\"\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIe3L-8LhAQw"
   },
   "source": [
    "### Pętla uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56417,
     "status": "ok",
     "timestamp": 1610371940016,
     "user": {
      "displayName": "Igor Sieradzki",
      "photoUrl": "",
      "userId": "10906567088243523558"
     },
     "user_tz": -60
    },
    "id": "xXEsqqvxhAQx",
    "outputId": "b9b9b88d-2753-4b5e-a013-9f6a1f9c7f04",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Progress:  0% Loss: 2.929\n",
      "Epoch: 0 Progress:  6% Loss: 2.866\n",
      "Epoch: 0 Progress: 11% Loss: 2.753\n",
      "Epoch: 0 Progress: 17% Loss: 2.468\n",
      "Epoch: 0 Progress: 22% Loss: 2.211\n",
      "Epoch: 0 Progress: 28% Loss: 2.089\n",
      "Epoch: 0 Progress: 33% Loss: 2.016\n",
      "Epoch: 0 Progress: 39% Loss: 1.880\n",
      "Epoch: 0 Progress: 44% Loss: 1.917\n",
      "Epoch: 0 Progress: 50% Loss: 1.803\n",
      "Epoch: 0 Progress: 55% Loss: 1.786\n",
      "Epoch: 0 Progress: 61% Loss: 1.791\n",
      "Epoch: 0 Progress: 66% Loss: 1.690\n",
      "Epoch: 0 Progress: 72% Loss: 1.683\n",
      "Epoch: 0 Progress: 77% Loss: 1.682\n",
      "Epoch: 0 Progress: 83% Loss: 1.640\n",
      "Epoch: 0 Progress: 89% Loss: 1.603\n",
      "Epoch: 0 Progress: 94% Loss: 1.665\n",
      "Epoch: 0 Progress: 100% Loss: 1.639\n",
      "Final F1 score: 0.19\n"
     ]
    }
   ],
   "source": [
    "n_class = len(label_to_idx)\n",
    "\n",
    "# initialize network and optimizer\n",
    "rnn = RNN(n_letters, 256, n_class)\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)   \n",
    "\n",
    "# we will train for only a single epoch \n",
    "epochs = 1\n",
    "\n",
    "\n",
    "# main loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss_buffer = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):  \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # get initial hidden state\n",
    "        hidden = rnn.init_hidden(x.shape[0])\n",
    "        \n",
    "        # get output for the sample, remember that we treat it as a sequence\n",
    "        # so you need to iterate over the 2nd, time dimensiotn\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        \n",
    "        output = ??? #najlepiej w pętli; uwaga RNN przy okazji generuje hidden\n",
    "        \n",
    "        loss = cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        loss_buffer.append(loss.item())\n",
    "        \n",
    "        if i % 1000 == 1:\n",
    "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
    "            loss_buffer = []\n",
    "    \n",
    "\n",
    "# evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    ps = []\n",
    "    ys = []\n",
    "    correct = 0\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        ys.append(y.numpy())\n",
    "\n",
    "        hidden = rnn.init_hidden(x.shape[0])\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        output = ???\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        ps.append(pred.cpu().numpy())\n",
    "    \n",
    "    ps = np.concatenate(ps, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    f1 = f1_score(ys, ps, average='weighted')\n",
    "    \n",
    "    print(f\"Final F1 score: {f1:.2f}\")\n",
    "    assert f1 > 0.15, \"You should get over 0.15 f1 score, try changing some hyperparams!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNeNU93qn7BC"
   },
   "source": [
    "<h4> Zadanie 2. </h4> \n",
    "\n",
    "Zaimplementuj funkcje `predict`, która przyjmuje nazwisko w postaci stringa oraz model RNN i wypisuje 3 najlepsze predykcje narodowości dla tego nazwiska razem z ich logitami.\n",
    "\n",
    "**Hint**: Przyda się tutaj jedna z funkcji z pierwszej komórki notebooka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N8FhF_08hAQy"
   },
   "outputs": [],
   "source": [
    "def predict(name: str, rnn: RNN):\n",
    "    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1610372364086,
     "user": {
      "displayName": "Igor Sieradzki",
      "photoUrl": "",
      "userId": "10906567088243523558"
     },
     "user_tz": -60
    },
    "id": "Z4OWP8wqhAQy",
    "outputId": "26f522d9-02f5-4b8a-eaa0-3af40141d15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satoshi\n",
      "\tjapanese: 3.86\n",
      "\tpolish: 2.97\n",
      "\titalian: 1.87\n",
      "Jackson\n",
      "\tscottish: 3.73\n",
      "\tenglish: 2.62\n",
      "\trussian: 2.33\n",
      "Schmidhuber\n",
      "\tgerman: 3.29\n",
      "\tdutch: 2.60\n",
      "\tenglish: 1.29\n",
      "Hinton\n",
      "\tscottish: 2.68\n",
      "\tpolish: 2.05\n",
      "\tenglish: 2.03\n",
      "Kowalski\n",
      "\tpolish: 5.67\n",
      "\tjapanese: 4.68\n",
      "\trussian: 3.06\n"
     ]
    }
   ],
   "source": [
    "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
    "\n",
    "for name in some_names:\n",
    "    print(name)\n",
    "    predict(name, rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNETvP06hAQz"
   },
   "source": [
    "<h4> Zadanie 3 </h4>\n",
    "\n",
    "Ostatnim zadaniem jest implementacji komórki i sieci LSTM. \n",
    "\n",
    "![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "* W klasie `LSTMCell` ma znaleźć się główna logika LSTMa, czyli wszystkie wagi do stanów `hidden` i `cell` jak i bramek kontrolujących te stany. \n",
    "* W klasie `LSTM` powinno znaleźć się wywołanie komórki LSTM, HINT: poprzednio było w pętli uczenia, teraz przenisiemy to do klasy modelu.\n",
    "* W pętli uczenia należy uzupełnić brakujące wywołania do uczenia i ewaluacji modelu.\n",
    "\n",
    "Zdecydowanie polecam [materiały Chrisa Olaha](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) do zarówno zrozumienia jak i ściągi do wzorów.\n",
    "\n",
    "Zadaniem jest osiągnięcie wartości `f1_score` lepszej niż na sieci RNN, przy prawidłowej implementacji nie powinno być z tym problemów używając podanych hiperparametrów. Dozwolona jest oczywiście zmiana `random seed`.\n",
    "\n",
    "#### Komórka LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GNKRxYwChAQz"
   },
   "outputs": [],
   "source": [
    "class LSTMCell(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "            Dimensionality of the input vector\n",
    "        :param hidden_size: int\n",
    "            Dimensionality of the hidden space\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # initialize LSTM weights \n",
    "        # NOTE: there are different approaches that are all correct \n",
    "        # (e.g. single matrix for all input opperations), you can pick\n",
    "        # whichever you like for this task\n",
    "    \n",
    "        ???\n",
    "\n",
    "    def forward(self, \n",
    "                input: torch.tensor, \n",
    "                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \n",
    "        hidden, cell = states\n",
    "        \n",
    "        # Compute input, forget, and output gates\n",
    "        # then compute new cell state and hidden state\n",
    "        # see http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "        \n",
    "        cell = ???\n",
    "        \n",
    "        hidden = ???\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U5U8kizhAQz"
   },
   "source": [
    "### Klasa modelu LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "G2MyIu3_hAQz"
   },
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "            Dimensionality of the input vector\n",
    "        :param hidden_size: int\n",
    "            Dimensionality of the hidden space\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.cell = LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
    "        \n",
    "    def forward(self, \n",
    "                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        :param input: torch.tensor \n",
    "            Input tesnor for a single observation at timestep t\n",
    "            shape [batch_size, input_size]\n",
    "        Returns Tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = input.shape[0]\n",
    "        \n",
    "        hidden, cell = self.init_hidden_cell(batch_size)\n",
    "        \n",
    "        hiddens = []\n",
    "        cells = []\n",
    "        \n",
    "        # this time we will process the whole sequence in the forward method\n",
    "        # as oppose to the previous exercise, remember to loop over the timesteps\n",
    "        \n",
    "        time_steps = input.shape[1]\n",
    "        \n",
    "        \n",
    "        #najlatwiej w pętli\n",
    "        hiddens = ???\n",
    "        \n",
    "        cells = ???\n",
    "        \n",
    "        return torch.stack(hiddens), torch.stack(cells)\n",
    "    \n",
    "    def init_hidden_cell(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns initial value for the hidden and cell states\n",
    "        \"\"\"\n",
    "        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True), \n",
    "                torch.zeros(batch_size, self.hidden_size, requires_grad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qRxPI-nhAQz"
   },
   "source": [
    "### Pętla uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147972,
     "status": "ok",
     "timestamp": 1610372306534,
     "user": {
      "displayName": "Igor Sieradzki",
      "photoUrl": "",
      "userId": "10906567088243523558"
     },
     "user_tz": -60
    },
    "id": "4LVCWqsVhAQ0",
    "outputId": "70088f64-dd7c-4713-bcf0-4d43e6fd9bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Progress:  0% Loss: 2.907\n",
      "Epoch: 0 Progress:  6% Loss: 2.689\n",
      "Epoch: 0 Progress: 11% Loss: 1.969\n",
      "Epoch: 0 Progress: 17% Loss: 1.880\n",
      "Epoch: 0 Progress: 22% Loss: 1.605\n",
      "Epoch: 0 Progress: 28% Loss: 1.507\n",
      "Epoch: 0 Progress: 33% Loss: 1.386\n",
      "Epoch: 0 Progress: 39% Loss: 1.366\n",
      "Epoch: 0 Progress: 44% Loss: 1.223\n",
      "Epoch: 0 Progress: 50% Loss: 1.178\n",
      "Epoch: 0 Progress: 55% Loss: 1.130\n",
      "Epoch: 0 Progress: 61% Loss: 1.103\n",
      "Epoch: 0 Progress: 66% Loss: 1.112\n",
      "Epoch: 0 Progress: 72% Loss: 1.013\n",
      "Epoch: 0 Progress: 77% Loss: 1.026\n",
      "Epoch: 0 Progress: 83% Loss: 0.980\n",
      "Epoch: 0 Progress: 89% Loss: 0.930\n",
      "Epoch: 0 Progress: 94% Loss: 0.890\n",
      "Epoch: 0 Progress: 100% Loss: 0.911\n",
      "Final F1 score: 0.20\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "# build data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# initialize the lstm with an additional cliassifier layer at the top\n",
    "lstm = LSTM(input_size=len(all_letters), hidden_size=128)\n",
    "clf = torch.nn.Linear(in_features=128, out_features=len(label_to_idx))\n",
    "\n",
    "# initialize a optimizer\n",
    "params = chain(lstm.parameters(), clf.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.01) \n",
    "\n",
    "# we will train for only a single epoch \n",
    "epoch = 1\n",
    "\n",
    "# main loop\n",
    "for epoch in range(epoch):\n",
    "    \n",
    "    loss_buffer = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):   \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # don't forget about the classifier!\n",
    "        \n",
    "        hidden, state = lstm(x)\n",
    "        output = ???\n",
    "        \n",
    "        # calucate the loss\n",
    "        loss = cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()                                \n",
    "        \n",
    "        loss_buffer.append(loss.item())\n",
    "        \n",
    "        if i % 1000 == 1:\n",
    "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
    "            loss_buffer = []\n",
    "\n",
    "# evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    \n",
    "    ps = []\n",
    "    ys = []\n",
    "    for i, (x, y) in enumerate(test_loader): \n",
    "        \n",
    "        ys.append(y.numpy())\n",
    "        \n",
    "        hidden, state = lstm(x)\n",
    "        output = ???\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        ps.append(pred.cpu().numpy())\n",
    "    \n",
    "    ps = np.concatenate(ps, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    f1 = f1_score(ys, ps, average='weighted')\n",
    "    \n",
    "    print(f\"Final F1 score: {f1:.2f}\")\n",
    "    assert f1 > 0.18, \"You should get over 0.18 f1 score, try changing some hiperparams!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGXUhgroo7AN"
   },
   "source": [
    "<h4> Zadanie 4. </h4>\n",
    "\n",
    "Zaimplementuj analogiczną do funkcji `predict` z zadania 2 dla modelu `lstm+clf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-ChJv1fphAQ0"
   },
   "outputs": [],
   "source": [
    "def predict_lstm(name: str, lstm: LSTM, clf: torch.nn.Module):\n",
    "    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1610372379028,
     "user": {
      "displayName": "Igor Sieradzki",
      "photoUrl": "",
      "userId": "10906567088243523558"
     },
     "user_tz": -60
    },
    "id": "pgQcGWqthAQ0",
    "outputId": "ce0d95db-27e4-46bf-cb89-49ad0a954aff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satoshi\n",
      "\tjapanese: 1.85\n",
      "\tarabic: 0.33\n",
      "\titalian: -3.87\n",
      "Jackson\n",
      "\tscottish: 0.91\n",
      "\tenglish: -0.54\n",
      "\tpolish: -3.11\n",
      "Schmidhuber\n",
      "\tarabic: 0.67\n",
      "\tirish: -1.35\n",
      "\tgerman: -1.69\n",
      "Hinton\n",
      "\tscottish: 1.26\n",
      "\tenglish: -0.76\n",
      "\tgerman: -3.15\n",
      "Kowalski\n",
      "\tpolish: 2.88\n",
      "\tczech: -2.03\n",
      "\trussian: -4.60\n"
     ]
    }
   ],
   "source": [
    "# test your lstm predictor\n",
    "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
    "    \n",
    "for name in some_names:\n",
    "    print(name)\n",
    "    predict_lstm(name, lstm, clf)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "laVdd5g5hAQu"
   ],
   "name": "12_RNN_SOLVED.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
